{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92377041",
   "metadata": {},
   "source": [
    "## 1. Importação de Bibliotecas\n",
    "Importação das bibliotecas fundamentais para o processo: `pandas` e `numpy` para manipulação de dados, e `sqlalchemy` para conexão e operações no banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34328f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d00a7",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Unificação dos Dados\n",
    "Leitura dos arquivos CSV contendo os dados do SAMU de 2023, 2024 e 2025. Em seguida, os dataframes são concatenados em um único (`df_unificado`), e são realizadas as conversões iniciais de tipos para as colunas de data, hora e idade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c121a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_nomes = [\n",
    "    'id', 'data', 'hora_minuto', 'municipio', 'bairro',\n",
    "    'endereco', 'origem_chamado', 'tipo', 'subtipo',\n",
    "    'sexo', 'idade', 'motivo_finalizacao', 'motivo_desfecho'\n",
    "]\n",
    "\n",
    "\n",
    "df_2025 = pd.read_csv('../data/samu_2025.csv', header=None, names=colunas_nomes)\n",
    "\n",
    "df_2024 = pd.read_csv('../data/samu_2024.csv', header=0, names=colunas_nomes)\n",
    "df_2023 = pd.read_csv('../data/samu_2023.csv', header=0, names=colunas_nomes)\n",
    "\n",
    "df_unificado = pd.concat([df_2025, df_2024, df_2023], ignore_index=True)\n",
    "\n",
    "df_unificado['id'] = df_unificado.index\n",
    "df_unificado.set_index('id', inplace=True)\n",
    "\n",
    "df_unificado['data'] = df_unificado['data'].astype(str).str.split('T').str[0]\n",
    "df_unificado['data'] = pd.to_datetime(df_unificado['data'], errors='coerce').dt.date\n",
    "\n",
    "df_unificado['hora_minuto'] = df_unificado['hora_minuto'].astype(str).str.strip().str[:8]\n",
    "df_unificado['hora_minuto'] = pd.to_datetime(df_unificado['hora_minuto'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "df_unificado['idade'] = pd.to_numeric(df_unificado['idade'], errors='coerce')\n",
    "\n",
    "\n",
    "print(df_unificado[['data', 'hora_minuto', 'idade']].info())\n",
    "df_unificado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a97d8",
   "metadata": {},
   "source": [
    "## 3. Verificação de Tipos de Dados\n",
    "Verificação pontual dos tipos de objetos nas colunas `DATA` e `HORA_MINUTO` para garantir que as conversões anteriores funcionaram como esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f199d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tipo real na coluna data:\", type(df_unificado['data'].iloc[0]))\n",
    "\n",
    "print(\"Tipo real na coluna hora:\", type(df_unificado['hora_minuto'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e87a187",
   "metadata": {},
   "source": [
    "## 4. Tratamento de Dados Faltantes: Idade\n",
    "Preenchimento dos valores nulos na coluna `IDADE` utilizando a mediana dos dados. Após o preenchimento, a coluna é convertida para o tipo inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76703348",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediana_idade = df_unificado['idade'].median()\n",
    "\n",
    "df_unificado['idade'].fillna(mediana_idade, inplace=True)\n",
    "df_unificado['idade'] = df_unificado['idade'].astype(int)\n",
    "\n",
    "qtd_nulos = df_unificado['idade'].isnull().sum()\n",
    "\n",
    "print(\"Quantidade de valores nulos na coluna idade após preenchimento:\", qtd_nulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe3838e",
   "metadata": {},
   "source": [
    "## 5. Tratamento de Nulos: Motivo de Finalização\n",
    "Substituição dos valores ausentes na coluna `MOTIVO_FINALIZACAO` pelo termo padronizado 'SEM FINALIZAÇÃO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00af74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unificado['motivo_finalizacao'] = df_unificado['motivo_finalizacao'].fillna('SEM FINALIZAÇÃO')\n",
    "\n",
    "print(\"Quantidade de nulos após tratamento:\", df_unificado['motivo_finalizacao'].isnull().sum())\n",
    "\n",
    "df_unificado[['motivo_finalizacao']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463114f",
   "metadata": {},
   "source": [
    "## 6. Tratamento de Nulos: Endereço e Origem\n",
    "Preenchimento de valores nulos nas colunas `ENDERECO` e `ORIGEM_CHAMADO` com o termo 'NÃO INFORMADO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unificado['endereco'] = df_unificado['endereco'].fillna('NÃO INFORMADO')\n",
    "\n",
    "df_unificado['origem_chamado'] = df_unificado['origem_chamado'].fillna('NÃO INFORMADO')\n",
    "\n",
    "print(df_unificado[['endereco', 'origem_chamado']].isnull().sum())\n",
    "\n",
    "df_unificado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebeb710",
   "metadata": {},
   "source": [
    "## 7. Tratamento de Nulos: Demais Colunas Categóricas\n",
    "Preenchimento massivo de valores nulos nas colunas restantes (`SEXO`, `SUBTIPO`, `TIPO`, `MUNICIPIO`, `BAIRRO`) com 'NÃO INFORMADO', garantindo que não restem campos vazios no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f107ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_restantes = ['sexo', 'subtipo', 'tipo', 'municipio', 'bairro']\n",
    "\n",
    "df_unificado[colunas_restantes] = df_unificado[colunas_restantes].fillna('NÃO INFORMADO')\n",
    "\n",
    "print(\"Contagem Final de Nulos:\")\n",
    "print(df_unificado.isnull().sum())\n",
    "\n",
    "df_unificado.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a75bf",
   "metadata": {},
   "source": [
    "## 8. Padronização de Textos\n",
    "Normalização das colunas de texto: conversão de todas as strings para letras maiúsculas e remoção de espaços em branco no início e fim (strip), facilitando agrupamentos futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_texto = ['municipio', 'bairro', 'endereco', 'origem_chamado', 'tipo', 'subtipo', 'sexo', 'motivo_finalizacao', 'motivo_desfecho']\n",
    "\n",
    "for col in colunas_texto:\n",
    "    df_unificado[col] = df_unificado[col].astype(str).str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901cf44",
   "metadata": {},
   "source": [
    "## 9. Inspeção de Valores Únicos\n",
    "Exibição dos valores únicos presentes em cada coluna de texto. Isso ajuda a identificar inconsistências de digitação (ex: 'Recife' vs 'RECIFE') que precisam de limpeza manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_texto = ['municipio', 'bairro', 'endereco', 'origem_chamado', 'tipo', 'subtipo', 'sexo', 'motivo_finalizacao', 'motivo_desfecho']\n",
    "\n",
    "for col in colunas_texto:\n",
    "    print(f\"\\nValores Únicos em {col}\")\n",
    "    valores = sorted(df_unificado[col].unique())\n",
    "    print(valores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3370b3b7",
   "metadata": {},
   "source": [
    "## 10. Limpeza Avançada: Origem do Chamado\n",
    "Correção de valores inconsistentes (\"sujos\") identificados na inspeção anterior na coluna `ORIGEM_CHAMADO`. Substitui termos inválidos por 'NÃO INFORMADO' e padroniza abreviações de estabelecimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_para_limpar = [\n",
    "    '93999830', 'ANI/ALI','JOSELENE', 'JUSELITA', \n",
    "    'MARCILIA', 'R MA','RAYSSA', 'R  CELIA','JAGUARIB' ,\n",
    "    'MONICA', 'AV NORTE', '00', 'MONIQUE', 'CARLOS', 'SANDRO',\n",
    "    'EDVALDO', 'RECIFE', 'EDIMILSO', 'MARIA', 'MANOEL R', 'TEC ENF',\n",
    "    'ANTONIO'\n",
    "]\n",
    "\n",
    "df_unificado['origem_chamado'] = df_unificado['origem_chamado'].replace(valores_para_limpar, 'NÃO INFORMADO')\n",
    "\n",
    "df_unificado['origem_chamado'] = df_unificado['origem_chamado'].replace('ESTAB PR', 'ESTABELECIMENTO PRIVADO')\n",
    "df_unificado['origem_chamado'] = df_unificado['origem_chamado'].replace('ESTAB PU', 'ESTABELECIMENTO PUBLICO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b12c53",
   "metadata": {},
   "source": [
    "## 11. Validação Pós-Limpeza\n",
    "Verificação simples para confirmar se a quantidade de nulos na coluna `MOTIVO_FINALIZACAO` foi zerada conforme planejado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos_finalizacao = df_unificado['motivo_finalizacao'].isnull().sum()\n",
    "print(\"Quantidade de valores nulos na coluna motivo_finalizacao após todas as correções:\", nulos_finalizacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fbb9b",
   "metadata": {},
   "source": [
    "## 12. Remoção de Linhas Duplicadas\n",
    "Eliminação de registros duplicados considerando um subconjunto de colunas chave. Exibe a contagem de linhas antes e depois para controle de qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_checagem = [\n",
    "    'data', 'hora_minuto', 'municipio', 'bairro',\n",
    "    'endereco', 'origem_chamado', 'tipo', 'subtipo',\n",
    "    'sexo', 'idade', 'motivo_finalizacao', 'motivo_desfecho'\n",
    "]\n",
    "\n",
    "qtd_antes = len(df_unificado)\n",
    "df_unificado.drop_duplicates(subset=colunas_checagem, keep='first', inplace=True)\n",
    "qtd_depois = len(df_unificado)\n",
    "\n",
    "print(f\"Linhas antes: {qtd_antes}\")\n",
    "print(f\"Linhas depois: {qtd_depois}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6472576",
   "metadata": {},
   "source": [
    "## 13. Engenharia de Atributos: Turno e Dia da Semana\n",
    "Criação de novas colunas analíticas:\n",
    "- `DIA_SEMANA`: Nome do dia em português.\n",
    "- `TURNO`: Categorização do horário (Manhã, Tarde, Noite, Madrugada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa_dias = {\n",
    "    'Monday': 'SEGUNDA-FEIRA', 'Tuesday': 'TERCA-FEIRA', 'Wednesday': 'QUARTA-FEIRA',\n",
    "    'Thursday': 'QUINTA-FEIRA', 'Friday': 'SEXTA-FEIRA', 'Saturday': 'SABADO', 'Sunday': 'DOMINGO'\n",
    "}\n",
    "\n",
    "df_unificado['data'] = pd.to_datetime(df_unificado['data'])\n",
    "df_unificado['dia_semana'] = df_unificado['data'].dt.day_name().map(mapa_dias)\n",
    "\n",
    "\n",
    "def definir_turno(hora_minuto):\n",
    "    try:\n",
    "        hora = int(str(hora_minuto)[:2])\n",
    "        \n",
    "        if 6 <= hora < 12:\n",
    "            return 'MANHA'\n",
    "        elif 12 <= hora < 18:\n",
    "            return 'TARDE'\n",
    "        elif 18 <= hora <= 23:\n",
    "            return 'NOITE'\n",
    "        else:\n",
    "            return 'MADRUGADA'\n",
    "    except:\n",
    "        return 'NAO INFORMADO'\n",
    "\n",
    "df_unificado['turno'] = df_unificado['hora_minuto'].apply(definir_turno)\n",
    "\n",
    "print(\"Novas colunas geradas:\")\n",
    "df_unificado[['data', 'dia_semana', 'hora_minuto', 'turno']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8541c",
   "metadata": {},
   "source": [
    "## 14. Engenharia de Atributos: Ano\n",
    "Extração do ano da data da ocorrência para uma coluna dedicada `ANO_ORIGEM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ba839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unificado['ano_origem'] = df_unificado['data'].dt.year.astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1ac42",
   "metadata": {},
   "source": [
    "## 15. Visualização dos Dados Tratados\n",
    "Exibição das primeiras linhas do dataframe final para conferência antes da carga no banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unificado.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139a219",
   "metadata": {},
   "source": [
    "## 16. Carga no Data Warehouse (ETL)\n",
    "Etapa final do processo:\n",
    "1. Conecta ao banco PostgreSQL.\n",
    "2. Limpa as tabelas existentes no esquema `dw_etl`.\n",
    "3. Cria e carrega as tabelas dimensão (`dim_localidade`, `dim_ocorrencia`, `dim_situacao`, `dim_paciente`, `dim_tempo`).\n",
    "4. Prepara a tabela fato (`fato_atendimentos`) realizando os *joins* necessários para obter as chaves estrangeiras (IDs).\n",
    "5. Carrega a tabela fato no banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1dfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga final no data warehouse do etl no esquema dw_etl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Configuracao da conexao\n",
    "DB_STRING = \"postgresql://postgres:admin123@localhost:5432/postgres\"\n",
    "engine = create_engine(DB_STRING)\n",
    "\n",
    "print(\"Iniciando carga no esquema dw_etl...\")\n",
    "\n",
    "# Limpeza das tabelas do esquema dw_etl antes de carregar\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"TRUNCATE TABLE dw_etl.fato_atendimentos CASCADE;\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dw_etl.dim_localidade CASCADE;\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dw_etl.dim_ocorrencia CASCADE;\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dw_etl.dim_situacao CASCADE;\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dw_etl.dim_paciente CASCADE;\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dw_etl.dim_tempo CASCADE;\"))\n",
    "    conn.commit()\n",
    "\n",
    "# Carga da dimensao localidade\n",
    "print(\"Carregando Dimensão Localidade...\")\n",
    "dim_local = df_unificado[['municipio', 'bairro']].drop_duplicates().sort_values(['municipio', 'bairro']).reset_index(drop=True)\n",
    "dim_local['id_local'] = dim_local.index + 1\n",
    "# Renomeia para minusculo para bater com o banco\n",
    "dim_local = dim_local.rename(columns={'municipio': 'municipio', 'bairro': 'bairro'})\n",
    "dim_local.to_sql('dim_localidade', engine, schema='dw_etl', if_exists='append', index=False)\n",
    "\n",
    "# Carga da dimensao ocorrencia\n",
    "print(\"Carregando Dimensão Ocorrência...\")\n",
    "dim_ocorrencia = df_unificado[['origem_chamado', 'tipo', 'subtipo']].drop_duplicates().sort_values(['tipo', 'subtipo']).reset_index(drop=True)\n",
    "dim_ocorrencia['id_ocorrencia'] = dim_ocorrencia.index + 1\n",
    "# Renomeia para minusculo\n",
    "dim_ocorrencia = dim_ocorrencia.rename(columns={'origem_chamado': 'origem_chamado', 'tipo': 'tipo', 'subtipo': 'subtipo'})\n",
    "dim_ocorrencia.to_sql('dim_ocorrencia', engine, schema='dw_etl', if_exists='append', index=False)\n",
    "\n",
    "# Carga da dimensao situacao\n",
    "print(\"Carregando Dimensão Situação...\")\n",
    "dim_situacao = df_unificado[['motivo_finalizacao', 'motivo_desfecho']].drop_duplicates().reset_index(drop=True)\n",
    "dim_situacao['id_situacao'] = dim_situacao.index + 1\n",
    "# Renomeia para minusculo\n",
    "dim_situacao = dim_situacao.rename(columns={'motivo_finalizacao': 'motivo_finalizacao', 'motivo_desfecho': 'motivo_desfecho'})\n",
    "dim_situacao.to_sql('dim_situacao', engine, schema='dw_etl', if_exists='append', index=False)\n",
    "\n",
    "# Carga da dimensao paciente\n",
    "print(\"Carregando Dimensão Paciente...\")\n",
    "# Cria dataframe temporario\n",
    "df_paciente_temp = df_unificado[['sexo', 'idade']].copy()\n",
    "bins = [-1, 12, 18, 59, 200]\n",
    "labels = ['CRIANCA', 'ADOLESCENTE', 'ADULTO', 'IDOSO']\n",
    "df_paciente_temp['faixa_etaria'] = pd.cut(df_paciente_temp['idade'], bins=bins, labels=labels).astype(str)\n",
    "\n",
    "# Remove duplicatas\n",
    "dim_paciente = df_paciente_temp[['sexo', 'faixa_etaria']].drop_duplicates().sort_values(['sexo']).reset_index(drop=True)\n",
    "dim_paciente['id_paciente'] = dim_paciente.index + 1\n",
    "# Renomeia para minusculo (sexo virou sexo)\n",
    "dim_paciente = dim_paciente.rename(columns={'sexo': 'sexo'})\n",
    "dim_paciente.to_sql('dim_paciente', engine, schema='dw_etl', if_exists='append', index=False)\n",
    "\n",
    "# Carga da dimensao tempo\n",
    "print(\"Carregando Dimensão Tempo...\")\n",
    "datas_unicas = pd.dataFrame({'data_completa': df_unificado['data'].unique()})\n",
    "# Converte para datetime\n",
    "datas_unicas['data_completa'] = pd.to_datetime(datas_unicas['data_completa'])\n",
    "\n",
    "datas_unicas['ano'] = datas_unicas['data_completa'].dt.year\n",
    "datas_unicas['mes'] = datas_unicas['data_completa'].dt.month\n",
    "datas_unicas['dia'] = datas_unicas['data_completa'].dt.day\n",
    "mapa_dias = {0:'SEGUNDA-FEIRA', 1:'TERCA-FEIRA', 2:'QUARTA-FEIRA', 3:'QUINTA-FEIRA', 4:'SEXTA-FEIRA', 5:'SABADO', 6:'DOMINGO'}\n",
    "datas_unicas['dia_semana'] = datas_unicas['data_completa'].dt.dayofweek.map(mapa_dias)\n",
    "datas_unicas['trimestre'] = datas_unicas['data_completa'].dt.quarter\n",
    "datas_unicas['semestre'] = np.where(datas_unicas['mes'] <= 6, 1, 2)\n",
    "\n",
    "dim_tempo = datas_unicas.sort_values('data_completa').reset_index(drop=True)\n",
    "dim_tempo['id_tempo'] = dim_tempo.index + 1\n",
    "# Converte para date\n",
    "dim_tempo['data_completa'] = dim_tempo['data_completa'].dt.date\n",
    "dim_tempo.to_sql('dim_tempo', engine, schema='dw_etl', if_exists='append', index=False)\n",
    "\n",
    "# Montagem e carga da tabela fato\n",
    "print(\"Montando e Carregando Tabela Fato...\")\n",
    "df_fato = df_unificado.copy()\n",
    "\n",
    "# Recalcula faixa etaria na fato\n",
    "df_fato['faixa_etaria'] = pd.cut(df_fato['idade'], bins=bins, labels=labels).astype(str)\n",
    "\n",
    "# Garante datetime para o merge\n",
    "df_fato['data'] = pd.to_datetime(df_fato['data'])\n",
    "\n",
    "# Merges usando as colunas originais maiusculas do df_fato\n",
    "df_fato = df_fato.merge(dim_local, left_on=['municipio', 'bairro'], right_on=['municipio', 'bairro'], how='left')\n",
    "df_fato = df_fato.merge(dim_ocorrencia, left_on=['origem_chamado', 'tipo', 'subtipo'], right_on=['origem_chamado', 'tipo', 'subtipo'], how='left')\n",
    "df_fato = df_fato.merge(dim_situacao, left_on=['motivo_finalizacao', 'motivo_desfecho'], right_on=['motivo_finalizacao', 'motivo_desfecho'], how='left')\n",
    "df_fato = df_fato.merge(dim_paciente, left_on=['sexo', 'faixa_etaria'], right_on=['sexo', 'faixa_etaria'], how='left')\n",
    "\n",
    "# Merge com tempo\n",
    "df_fato['data_join'] = df_fato['data'].dt.date\n",
    "df_fato = df_fato.merge(dim_tempo, left_on='data_join', right_on='data_completa', how='left')\n",
    "\n",
    "# Selecao das colunas finais\n",
    "df_fato_final = pd.dataFrame()\n",
    "df_fato_final['fk_local'] = df_fato['id_local']\n",
    "df_fato_final['fk_ocorrencia'] = df_fato['id_ocorrencia']\n",
    "df_fato_final['fk_situacao'] = df_fato['id_situacao']\n",
    "df_fato_final['fk_paciente'] = df_fato['id_paciente']\n",
    "df_fato_final['fk_tempo'] = df_fato['id_tempo']\n",
    "df_fato_final['hora_exata'] = df_fato['hora_minuto']\n",
    "df_fato_final['idade_paciente'] = df_fato['idade']\n",
    "df_fato_final['qtd_atendimentos'] = 1\n",
    "\n",
    "# Carga em lotes\n",
    "df_fato_final.to_sql('fato_atendimentos', engine, schema='dw_etl', if_exists='append', index=False, chunksize=2000)\n",
    "\n",
    "print(\"Carga ETL concluída no esquema dw_etl!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
